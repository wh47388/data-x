{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)\n",
    "\n",
    "\n",
    "## Prediction \n",
    "\n",
    "Author List: \n",
    "\n",
    "Sana Iqbal, significant edits for fall 2017\n",
    "\n",
    "Kevin Li, Ikhlaq Sidhu, Spring 2017\n",
    "\n",
    "Original Sources: http://scikit-learn.org,http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "License: Feel free to do whatever you want to with this code\n",
    "\n",
    "\n",
    "### Our  predictive machine learning models perform two types of tasks:\n",
    "\n",
    "* __CLASSIFICATION__:\n",
    "LABELS ARE DISCRETE VALUES.\n",
    "Here the model is trained to classify each instance into a set of predefined  discrete classes.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a  class of that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __defaulter or non-defaulter__ as labels. When we input income and expenditure data  of any customer in this model, it will predict whether the customer is going to default or not.\n",
    "\n",
    "* __REGRESSION__:\n",
    "LABELS ARE CONTINUOUS VALUES.\n",
    "Here the model is trained to predict a continuous value for each instance.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a continuous value  for  that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __ default amount__ as the label. This model when input with income and expenditure data of any customer will be able to predict the default amount the customer might end up with.\n",
    "\n",
    "\n",
    "* __TO GET STARTED:__:\n",
    "\n",
    "We will use python library -SCIKIT-LEARN for our classification and regression models.\n",
    "\n",
    "1. Install numpy, scipy, scikit-learn.\n",
    "\n",
    "2. Download the dataset provided and save it in your current working directory.\n",
    "\n",
    "3. In the following sections  you will:\n",
    "\n",
    "    3.1 Read the dataset into the python program.\n",
    "    \n",
    "    3.2 Look  into the dataset characteristics, check for feature type - categorical or numerical.\n",
    "    \n",
    "    3.3 Find feature distributions to check sufficiency of data.\n",
    "    \n",
    "    3.4 Divide the dataset into training and validation subsets.\n",
    "    \n",
    "    3.5 Fit models with training data  using scikit-learn library.\n",
    "    \n",
    "    3.6 Calculate training error,this gives you the idea of bias in your model.\n",
    "    \n",
    "    3.7 Test model prediction accuracy using validation data,this gives you bias and variance error in the model.\n",
    "    \n",
    "    3.8 Report model performance on validation data using different metrics.\n",
    "    \n",
    "    3.9 Save the model parameters in a pickle file so that it can be used for test data.\n",
    "    \n",
    "  Also, if our data set is small we will have fewer examples for validation.\n",
    "This will not give us a a good estimatiion of model error.\n",
    "We can use  k-fold crossvalidation in such situations.\n",
    "In k-fold cross-validation, the shuffled training data is partitioned into k disjoint sets and the model is trained on k âˆ’1 sets and validated on the kth set. This process is repeated k times with each set\n",
    "chosen as the validation set once. The cross-validation accuracy is reported as the average accuracy\n",
    "of the k iterations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSUMPTIONS:\n",
    "### > Problem:\n",
    "'Iris 'setosa' species has medicinal benefits and we want to make the process of identifying an iris species scalable'  \n",
    "\n",
    "### > Type: \n",
    "Classification\n",
    "### > Data : \n",
    "Flower morphology data collected by floriculture department\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path='iris_classification.csv'\n",
    "data=pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets us look at the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets us check unique labels:\n",
    "data['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# SHUFFLE data instances to randomize the distribution of different classes\n",
    "# Check if data has any NAN  values, you can choose to drop NAN \n",
    "# containing rows or replace NAN  values with mean. median,or any assumed value.\n",
    "\n",
    "data= shuffle(data).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of NaNs in the dataframe:\\n',data.isnull().sum())\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our functions take in features and labels as arrays  so I can separate them :GET FEATURES FROM THE DATA\n",
    "X=data.iloc[:,:-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GET LABELS FROM THE DATA\n",
    "Y=data['species']\n",
    "\n",
    "\n",
    "print (Y.value_counts()) #gives the count of each label in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print ('''\\nWe will map our class labels to integers and then use in modeling.\n",
    "The mapping is:'versicolor': 0, 'virginica': 1,'setosa' :2 \\n''')\n",
    "\n",
    "Y=Y.map({'versicolor': 0, 'virginica': 1,'setosa' :2})\n",
    "print (Y.value_counts()) #gives the count of each label in the dataset\n",
    "\n",
    "Y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should do sanity check on data often\n",
    "print(\"Feature vector shape=\", X.shape)\n",
    "print(\"Class shape=\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More summary about our data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get feature distribution of each ontinuous valued feature\n",
    "data.hist(figsize=(15,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check feature distribution of each class, to get an overview of feature and class relationshhip,\n",
    "# also useful in validating data\n",
    "print(data.groupby('species').count())\n",
    "\n",
    "\n",
    "data.groupby('species').hist(figsize=(10,5))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Hypothesis : Species of Iris is dependent on sepal length and width of the flower.\n",
    "### USE SKLEARN INBUILT FUNCTION TO BUILD A LOGISTIC REGRESSION MODEL  \n",
    "For Details check :\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "\n",
    "In order to check the validity of our trained model, we keep a part of our dataset hidden from the model during training, called  __Validation data__.\n",
    "\n",
    "Validation data labels are predicted using the trained model and compared with the actual labels of the data.This gives us the idea about how well the model can be trusted for its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation set  using sklearn function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Name our logistic regression object\n",
    "LogisticRegressionModel = linear_model.LogisticRegression()\n",
    "\n",
    "# we create an instance of logistic Regression Classifier and fit the data.\n",
    "print ('Training a logistic Regression Model..')\n",
    "LogisticRegressionModel.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model -----Training Accuracy and Test Accuracy\n",
    "##  \n",
    "#### TRAINING ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING ACCURACY\n",
    "\n",
    "training_accuracy=LogisticRegressionModel.score(x_train,y_train)\n",
    "print ('Training Accuracy:',training_accuracy)\n",
    "#or\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets us see how it is done ab-initio. Estimate prediction error logically for each sample \n",
    "# and save it in an array called Loss_Array\n",
    "\n",
    "\n",
    "# this line below will predict a category for every row in x_train\n",
    "predicted_label = LogisticRegressionModel.predict(x_train) \n",
    "\n",
    "    \n",
    "\n",
    "def find_error(actual_label,predicted_label):\n",
    "    '''actual_label= label in data\n",
    "        predicted_label= label predicted by the model\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    Loss_Array = np.zeros(len(actual_label)) #create an empty array to store loss values\n",
    "#     print(Loss_Array)\n",
    "    \n",
    "    for i,value in enumerate(actual_label):\n",
    "        \n",
    "        if value == predicted_label[i]: \n",
    "            Loss_Array[i] = 0\n",
    "        else:\n",
    "            Loss_Array[i] = 1\n",
    "\n",
    "    print (\"Y-actualLabel   Z-predictedLabel   Error \\n\")\n",
    "    for i,value in enumerate(actual_label):\n",
    "        print (value,\"\\t\\t\" ,predicted_label[i],\"\\t\\t\",Loss_Array[i])\n",
    "        \n",
    "    error_rate=np.average(Loss_Array)\n",
    "    print (\"\\nThe error rate is \", error_rate)\n",
    "    print ('\\nThe accuracy of the model is ',1-error_rate )\n",
    "    \n",
    "    \n",
    "find_error(y_train,predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  VALIDATION ACCURACY: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VALIDATION ACCURACY: \n",
    "# we will find accuracy of the model \n",
    "# using data that was not used for training the model\n",
    "\n",
    "validation_accuracy=LogisticRegressionModel.score(x_test,y_test)\n",
    "print('Accuraacy of the model on unseen validation data: ',validation_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = y_test\n",
    "y_pred = LogisticRegressionModel.predict(x_test)\n",
    "ConfusionMatrix=pd.DataFrame(confusion_matrix(y_true, y_pred),columns=['Predicted 0','Predicted 1','Predicted 2'],index=['Actual 0','Actual 1','Actual 2'])\n",
    "print ('Confusion matrix of test data is: \\n',ConfusionMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PLOT THE DECISION BOUNDARIES:\n",
    "# 1.create meshgrid of all points between \n",
    "\n",
    "'''\n",
    "For that we will create a mesh between [x_min, x_max]x[y_min, y_max].\n",
    "We will choose a 2d vector space ranging from values +- 0.5 from our \n",
    "min and max values of sepal_length and sepal_width.\n",
    "Then we will divide that whole region in a grid of  0.02 units cell size.\n",
    "\n",
    "'''\n",
    "\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min = X['sepal_length'].min() - .5\n",
    "x_max = X['sepal_length'].max() + .5\n",
    "y_min = X['sepal_width'].min() - .5\n",
    "y_max = X['sepal_width'].max() + .5\n",
    "\n",
    "\n",
    "\n",
    "# print x_min, x_max, y_min, y_max\n",
    "\n",
    "sepal_length_range = np.arange(x_min, x_max, h)\n",
    "sepal_width_range = np.arange(y_min, y_max, h)\n",
    "\n",
    "\n",
    "# Create datapoints for the mesh\n",
    "sepal_length_values, sepal_width_values = np.meshgrid(sepal_length_range, sepal_width_range)\n",
    "\n",
    "\n",
    "# Predict species for the fictious data in meshgrid\n",
    "predicted_species = LogisticRegressionModel.predict(np.c_[sepal_length_values.ravel(), sepal_width_values.ravel()])\n",
    "\n",
    "\n",
    "print ('Finished predicting species')\n",
    "\n",
    "# another approach is to make an array Z2 which has all the predicted values in (xr,yr).  \n",
    "\n",
    "predicted_species2= np.arange(len(sepal_length_range)*len(sepal_width_range)).reshape(len(sepal_length_range),len(sepal_width_range))\n",
    "for yni in range(len(sepal_width_range)):\n",
    "    for xni in range(len(sepal_length_range)):\n",
    "#         print (xni, yni, LogisticRegressionModel.predict([[xr[xni],yr[yni]]]))\n",
    "\n",
    "        predicted_species2[xni,yni] =LogisticRegressionModel.predict([[sepal_length_range[xni],sepal_width_range[yni]]])\n",
    "print ('Finished predicted_species2')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "predicted_species = predicted_species.reshape(sepal_length_values.shape)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.pcolormesh(sepal_length_values,sepal_width_values,predicted_species , cmap=plt.cm.Paired)\n",
    "plt.title('Decision Boundaries and Class Regions identified by the trained model ')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(15,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(sepal_length_values,sepal_width_values,predicted_species , cmap=plt.cm.Paired)\n",
    "\n",
    "plt.scatter(X['sepal_length'], X['sepal_width'], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.colorbar()\n",
    "plt.xlim(sepal_length_values.min(),sepal_length_values.max())\n",
    "plt.ylim(sepal_width_values.min(), sepal_width_values.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('ACTUAL data points on colored decision regions of the model ')\n",
    "\n",
    "# Put the result into a color plot of decison boundary\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('PREDICTED labels of points on colored decision regions of the model ')\n",
    "plt.pcolormesh(sepal_length_values,sepal_width_values,predicted_species, cmap=plt.cm.Paired)\n",
    "plt.colorbar()\n",
    "label=np.unique(y_test)\n",
    "\n",
    "plt.scatter(x_train['sepal_length'], x_train['sepal_width'], c=pd.DataFrame(predicted_label), edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(sepal_length_values.min(),sepal_length_values.max())\n",
    "plt.ylim(sepal_width_values.min(), sepal_width_values.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(15,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(sepal_length_values,sepal_width_values,predicted_species, cmap=plt.cm.Paired)\n",
    "plt.colorbar()\n",
    "label=np.unique(y_test)\n",
    "plt.title('VALIDATION DATA -ACTUAL LABELS')\n",
    "# Plot also the training points\n",
    "plt.scatter(x_test['sepal_length'], x_test['sepal_width'], c=y_test,label=np.unique(y_test), edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(sepal_length_values,sepal_width_values,predicted_species, cmap=plt.cm.Paired)\n",
    "plt.colorbar()\n",
    "# Plot also the training points\n",
    "plt.scatter(x_test['sepal_length'], x_test['sepal_width'], c=LogisticRegressionModel.predict(x_test), edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('VALIDATION DATA -PREDICTED LABELS')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Example\n",
    "\n",
    "__Linear regression__ is a predictive modeling technique for predicting a numeric response variable based on features.  \n",
    "\"Linear\" in the name linear regression refers to the fact that this method fits a model where response bears linear relationship with features. (ie Z is proportional to first power of x)\n",
    "\n",
    "__Z = X0 + a(X1) + b(X2) +.... where:__   \n",
    "Z: predicted response  \n",
    "X0: intercept  \n",
    "a,b,..: Coefficients of X1,X2..  \n",
    "\n",
    "If Y is the actual response and Z is the predicted response,    \n",
    "__Y-Z= Residual__  \n",
    "Average Residual defines model performance,residual equal to zero represents a perfect fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Source: Scikit learn\n",
    "Code source: Jaques Grobler\n",
    "License: BSD 3 clause'''\n",
    "from sklearn import linear_model\n",
    "\n",
    "example_dff = pd.DataFrame(np.random.randint(0,100,size=(100, 1)),columns=['X'])\n",
    "example_dff['C']=5.1*example_dff['X']\n",
    "# example_dff['C']=5.1*example_dff['X']**2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_reg=example_dff[['X']]\n",
    "\n",
    "Y_reg=example_dff['C']\n",
    "\n",
    "# Create linear regression object\n",
    "LinearRegressionModel= linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "LinearRegressionModel.fit(X_reg, Y_reg)\n",
    "Z_reg=LinearRegressionModel.predict(X_reg)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', LinearRegressionModel.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\",np.mean((Z_reg - Y_reg) ** 2))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_reg['X'], Y_reg,  color='red')\n",
    "plt.plot(X_reg['X'], Z_reg, color='blue',\n",
    "         linewidth=3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression using data with one feature -X')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
