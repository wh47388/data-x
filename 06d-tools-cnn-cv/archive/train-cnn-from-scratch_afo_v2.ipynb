{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Train CNN from Scratch CATS vs DOGS \n",
    "#### Small training sample convolutional neural net with data augmentation\n",
    "\n",
    "**Author:** Alexander Fred Ojala\n",
    "\n",
    "**Sources:** \n",
    "* **Data**: https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition (subset, note all images are unique cat and dog photos)\n",
    "* **Training + explanations**: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "**Copright:** Feel free to do whatever you want with this code.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required data\n",
    "\n",
    "## Downlad vgg16_weights.h5:\n",
    "From here: https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document outline (clickable links)\n",
    "___\n",
    "\n",
    "### [0: Image data logistics & package imports](#sec0)\n",
    "\n",
    "#### **RUN THIS SECTION FIRST!**\n",
    "In part 0 we read in the required modules, libraries and packages. We define varibales used throughout in the analysis, look at the data structure (train, validation, test, and try sets).\n",
    "\n",
    "\n",
    "### [1: Training a small convnet from scratch: 80% accuracy in 40 lines of code](#sec1)\n",
    "\n",
    "Here we show how to implement a CNN from scratch and train it using only 2000 images (1000 per class). Takes about ~2.5hrs to run from scratch, this is pre-run for you if you want to play with it. We also show how Keras can augment a picture to never show the same picture twice to the CNN.\n",
    "\n",
    "* **Model filename:** mod_appendix.json\n",
    "* **Weights filename:** w_appendix.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>\n",
    "# Part 0\n",
    "## Image data logistics and package imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK-data-x_breakout7_cnn.ipynb\r\n",
      "SOLUTIONS-NOTEBOOK-data-x_breakout7_cnn.ipynb\r\n",
      "\u001b[34mdata\u001b[m\u001b[m\r\n",
      "features_test.npy\r\n",
      "features_train.npy\r\n",
      "features_validation.npy\r\n",
      "mod_appendix.json\r\n",
      "w_appendix.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>The files you should have are:</center>\n",
    "\n",
    "| Files                               |  Files                  |\n",
    "| ----------------------------------- | ----------------------- |\n",
    "| NOTEBOOK-data-x_breakout7_cnn.ipynb | features_validation.npy |\n",
    "| data                                | mod_appendix.json       |\n",
    "| features_test.npy                   | vgg16_weights.h5        |\n",
    "| features_train.npy                  | w_appendix.h5           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLDER ./data\n",
      "FOLDER ./data/test\n",
      "FOLDER ./data/test/catvdog\n",
      "try001.jpg\n",
      "try002.jpg\n",
      "try003.jpg\n",
      "try004.jpg\n",
      "FOLDER ./data/train\n",
      "FOLDER ./data/train/cats\n",
      "cat0001.jpg\n",
      "cat0002.jpg\n",
      "cat0003.jpg\n",
      "cat0004.jpg\n",
      "FOLDER ./data/train/dogs\n",
      "dog0001.jpg\n",
      "dog0002.jpg\n",
      "dog0003.jpg\n",
      "dog0004.jpg\n",
      "FOLDER ./data/validation\n",
      "FOLDER ./data/validation/cats\n",
      "cat001001.jpg\n",
      "cat001002.jpg\n",
      "cat001003.jpg\n",
      "cat001004.jpg\n",
      "FOLDER ./data/validation/dogs\n",
      "dog001001.jpg\n",
      "dog001002.jpg\n",
      "dog001003.jpg\n",
      "dog001004.jpg\n"
     ]
    }
   ],
   "source": [
    "# Look at files, note all cat images and dog images are unique\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "for path, dirs, files in os.walk('./data'):\n",
    "    print('FOLDER',path)\n",
    "    for f in files[:4]:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cat training images: 1000\n",
      "Number of dog training images: 1000\n",
      "Number of cat validation images: 400\n",
      "Number of dog validation images: 400\n",
      "Number of uncategorized test images: 100\n"
     ]
    }
   ],
   "source": [
    "print('Number of cat training images:', len(os.walk('./data/train/cats').next()[2]))\n",
    "print('Number of dog training images:', len(os.walk('./data/train/dogs').next()[2]))\n",
    "print('Number of cat validation images:', len(os.walk('./data/validation/cats').next()[2]))\n",
    "print('Number of dog validation images:', len(os.walk('./data/validation/dogs').next()[2]))\n",
    "print('Number of uncategorized test images:', len(os.walk('./data/test/catvdog').next()[2]))\n",
    "\n",
    "# There should be 1000 train cat images, 1000 train dogs, 400 validation cats, 400 validation dogs, 100 uncategorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variables\n",
    "TRAIN_DIR = './data/train/'\n",
    "VAL_DIR = './data/validation/'\n",
    "TEST_DIR = './data/test/' #one mixed category\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "n_train_samples = 2000\n",
    "n_validation_samples = 800\n",
    "n_epoch = 30\n",
    "n_test_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Set theano backend\n",
    "# see ~/.keras/keras.json and / or https://keras.io/backend/#switching-from-one-backend-to-another\n",
    "!KERAS_BACKEND=theano python -c \"from keras import backend\"\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import h5py\n",
    "import os, cv2, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation, ZeroPadding2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "# 1: Training a small convnet from scratch: 80% accuracy in 40 lines of code\n",
    "\n",
    "The right tool for an image classification job is a convnet, so let's try to train one on our data, as an initial baseline. Since we only have few examples, our number one concern should be overfitting. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three, images of people who are sailors, and among them only one lumberjack wears a cap, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n",
    "\n",
    "Data augmentation is one way to fight overfitting, but it isn't enough since our augmented samples are still highly correlated. Your main focus for fighting overfitting should be the entropic capacity of your model --how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.\n",
    "\n",
    "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.\n",
    "\n",
    "In our case we will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).\n",
    "\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers. This is very similar to the architectures that Yann LeCun advocated in the 1990s for image classification (with the exception of ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.\n",
    "\n",
    "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class. This class allows you to:\n",
    "\n",
    "configure random transformations and normalization operations to be done on your image data during training\n",
    "instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs, fit_generator, evaluate_generator and predict_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import image data generator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40, #rotation_range degrees (0-180), range that randomly rotate pictures\n",
    "        width_shift_range=0.2, #width_shift range (fraction of total width) within which to randomly translate pic\n",
    "        height_shift_range=0.2, # -ii-\n",
    "        \n",
    "        #rescale value we multiply the data before any other processing. \n",
    "        #Our original images consist in RGB coefficients in the 0-255, \n",
    "        #but such values would be too high for our models to process (given typical learning rate), \n",
    "        # so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "        rescale=1./255,\n",
    "        \n",
    "        #randomly applying shearing transformations (shear mapping is a linear map that \n",
    "        #displaces each point in fixed direction, by an amount proportional to its \n",
    "        #signed distance from a line that is parallel to that direction)\n",
    "        shear_range=0.2, \n",
    "        zoom_range=0.2, #randomly zooming inside pictures\n",
    "        \n",
    "        #is for randomly flipping half of the images horizontally \n",
    "        #--relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n",
    "\n",
    "        horizontal_flip=True,\n",
    "    \n",
    "        #is the strategy used for filling in newly created pixels, \n",
    "        #which can appear after a rotation or a width/height shift.\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start generating some pictures using this tool and save them to a temporary directory, so we can get a feel for what our augmentation strategy is doing --we disable rescaling in this case to keep the images displayable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = load_img(TRAIN_DIR+'cats/cat0001.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "\n",
    "if not os.path.exists('preview'):\n",
    "    os.makedirs('preview')\n",
    "\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely\n",
    "\n",
    "prev_files = os.walk('./preview').next()[2]\n",
    "print(prev_files[:4])\n",
    "\n",
    "for img in prev_files[1:4]:\n",
    "    print('Image '+img)\n",
    "    display(Image(filename='preview/'+img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=(3, img_height, img_width)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    '''\n",
    "    On top of it we stick two fully-connected layers. \n",
    "    We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. \n",
    "    To go with it we will also use the binary_crossentropy loss to train our model.\n",
    "    '''\n",
    "\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Let's prepare our data. We will use .flow_from_directory()\n",
    "    # to generate batches of image data (and their labels) directly from our jpgs in their respective folders.\n",
    "    \n",
    "    # Below is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "    # this is the augmentation configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolers of 'data/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    print('Train generator')\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            TRAIN_DIR,  # this is the target directory\n",
    "            target_size=(img_height, img_width),  # all images will be resized to 150x150\n",
    "            batch_size=32,\n",
    "            class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "    # this is a similar generator, for validation data\n",
    "    print('Validation generator')\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "            VAL_DIR,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=32,\n",
    "            class_mode='binary')\n",
    "    \n",
    "\n",
    "    \n",
    "    return model, train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at class indices from our generators\n",
    "\n",
    "\n",
    "_, train_gen,val_gen =first_model()\n",
    "print('')\n",
    "print(val_gen.class_indices)\n",
    "print(val_gen.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define and fit the first model\n",
    "n_epoch = 50\n",
    "def fit_first_model():\n",
    "\n",
    "    mod1, train_generator, validation_generator = first_model()\n",
    "    mod1.fit_generator(\n",
    "            train_generator,\n",
    "            samples_per_epoch=n_train_samples,\n",
    "            nb_epoch=n_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            nb_val_samples=n_validation_samples)\n",
    "\n",
    "    # save model to disk\n",
    "    mod1.save_weights('w_appendix.h5')  # always save your weights after training or during training\n",
    "    model_json = mod1.to_json()\n",
    "    with open(\"mod_appendix.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "#fit_first_model()\n",
    "\n",
    "\n",
    "### DONE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIRST MODEL EXPLORATION\n",
    "\n",
    "# load model 1 and weights\n",
    "\n",
    "json_file = open('mod_appendix.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "mod1 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "mod1.load_weights(\"w_appendix.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "mod1.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Extract image features from test set - to make predictions\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "val_generator = datagen.flow_from_directory( VAL_DIR, target_size=(img_height, img_width),\n",
    "                                              batch_size=32,class_mode='binary')\n",
    "\n",
    "preds = mod1.evaluate_generator(val_generator,n_validation_samples)\n",
    "\n",
    "print('\\nModel 1 accuracy on 800 validation images:', round(sum(preds)/2,4)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot picture and print class prediction on cats vs dogs (unsorted)\n",
    "\n",
    "\n",
    "try_images =  [TEST_DIR+'catvdog/'+img for img in os.listdir(TEST_DIR+'catvdog/')]\n",
    "\n",
    "def read_image(file_path):\n",
    "    # For image visualization\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    return cv2.resize(img, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def plot_pic(img):\n",
    "    # Plot openCV pic\n",
    "    pic = read_image(img)    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()\n",
    "\n",
    "def predict(mod,i=0,r=None):\n",
    "    if r==None:\n",
    "        r=[i]\n",
    "        \n",
    "    for idx in r:\n",
    "        \n",
    "        img_path = try_images[idx]\n",
    "        img = image.load_img(img_path, target_size=(150, 150))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        class_pred = mod.predict_classes(x,verbose=0)\n",
    "        \n",
    "        if class_pred == 0:\n",
    "            class_guess='CAT'\n",
    "        else:\n",
    "            class_guess='DOG'\n",
    "        \n",
    "        print('\\n\\nI think this is a ' + class_guess)\n",
    "        plot_pic(try_images[idx])\n",
    "\n",
    "predict(mod1,r=range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
