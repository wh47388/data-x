{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)\n",
    "\n",
    "## NLTK code examples\n",
    "Python code examples to mirror lecture material\n",
    "\n",
    "**Author List**: Sam Choi\n",
    "\n",
    "**Original Sources**: http://nltk.org\n",
    "\n",
    "**License**: Feel free to do whatever you want to with this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing NLTK and a couple sets of data. We'll import corpora previously downloaded through nltk, and start exploring the Project Gutenberg corpus - an archive of over 50,000 ebooks.\n",
    "\n",
    "*for more info on accessing corpora: http://www.nltk.org/book/ch02.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg, shakespeare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Gutenberg\n",
    "\n",
    "Different corpora have different information available. Let's explore some of the functions that we highlighted using Jane Austen's *Emma*.  \n",
    "\n",
    "sents() tokenizes a text file into multiple lists of words - each list contains the words in a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = gutenberg.sents('austen-emma.txt')\n",
    "\n",
    "print(\"Sentence: \" + str(sentences[1500]) + \"\\n\")\n",
    "print(\"Number of sentences: \" + str(len(sentences)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, words() tokenizes a text file into mutiple lists containing single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = gutenberg.words('austen-emma.txt')\n",
    "print(\"Word: \" + str(words[50000]) + \"\\n\")\n",
    "print(\"Number of words: \" + str(len(words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout\n",
    "\n",
    "1. Download and import the gutenberg corpus\n",
    "2. Calculate the number of sentences and words in each corpus\n",
    "3. Find the corpus with the most words\n",
    "4. Find the corpus with the most sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import gutenberg\n",
    "\n",
    "all_files = gutenberg.fileids()\n",
    "for file in all_files:\n",
    "    print(file)\n",
    "\n",
    "# print(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Shakespeare\n",
    "\n",
    "Here we'll being playing around with the Shakespeare corpus - a corpus containing a set of Shakespeare's plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shakespeare.fileids()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've printed a list of file names for each play in the Shakespeare corpus. Let's compare the lengths of each of these plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for play in shakespeare.fileids():\n",
    "    words = shakespeare.words(play)\n",
    "    print(play + \": \" + str(len(words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play = shakespeare.xml('r_and_j.xml')\n",
    "print (play[0].tag + \": \" + play[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has already categorized words in the Shakespeare corpus with certain tags. One of these tags is 'PERSONAE/PERSONA', which marks words that are related to characters in Shakespeare's plays. Let's use this to list the characters in the play, and count how many roles are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "people = []\n",
    "for person in play.findall(\"PERSONAE/PERSONA\"):\n",
    "    people.append(person.text)\n",
    "\n",
    "for person in people:\n",
    "    print(person)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Cast size: \" + str(len(people)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter\n",
    "\n",
    "Now let's explore another interesting corpus called twitter_samples: a sample of a couple thousand tweets from twitter's global feed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter_samples is partitioned into positive_tweets and negative_tweets. We'll use this fact to make some basic comparisons between positive and negative tweets.\n",
    "\n",
    "(include link to twitter_samples documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "happy = twitter_samples.tokenized('positive_tweets.json')\n",
    "sad = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "print(\"Happy tweets: \" + str(len(happy)))\n",
    "print(\"Sad tweets: \" + str(len(sad)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the frequency of some words that might be used in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sad_omg = []\n",
    "for tweet in sad:\n",
    "    if \"omg\" in tweet:\n",
    "        sad_omg.append(tweet)\n",
    "\n",
    "print(\"sad omg count: \" + str(len(sad_omg)))\n",
    "\n",
    "happy_omg = []\n",
    "for tweet in happy:\n",
    "    if \"omg\" in tweet:\n",
    "        happy_omg.append(tweet)\n",
    "\n",
    "print(\"Happy Tweets: \" + str(happy_omg))\n",
    "\n",
    "count = 1\n",
    "for tweet in happy_omg:\n",
    "    tw = \" \".join(tweet)\n",
    "    print(\"Tweet \" + str(count) + \": \" + str(tw) + \"\\n\")\n",
    "    count += 1\n",
    "\n",
    "print(\"happy omg count: \" + str(len(happy_omg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on the dataset provided by the twitter_samples corpus, sad tweets seem to contain the phrase \"omg\" more frequently than happy tweets.\n",
    "\n",
    "This of course is a very basic example of analyzing twitter data - NLTK provides a very powerful set of tools that can be used for many other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Tweets  \n",
    "\n",
    "Now that we've seen what's possible with NLTK and the twitter_samples corpus, let's create our own metric for classifying positive and negative tweets (we'll keep it relatively simple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered two of the files in the twitter_samples corpus, but lets see all of the filenames to see the other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(twitter_samples.fileids())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the third file is called 'tweets.20150430-223406.json'. This happens to be a collection of tweets from 4/30/2015 that have yet to be classified. This means that for all intents and purposes, these tweets are pretty much a random sampling of the twitterverse.\n",
    "\n",
    "Let's go ahead and unpack the tweets in this file like we did for the positive/negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_tweets = twitter_samples.tokenized('tweets.20150430-223406.json')\n",
    "print(\"Random tweets: \" + str(len(random_tweets)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our sample contains 20,000 unclassified tweets. In order to classify them, let's create two lists of keywords/phrases that we'll be searching for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happy_indicators = []   #fill with happy strings\n",
    "sad_indicators = []   #fill with sad strings\n",
    "\n",
    "# ex: \n",
    "# happy = [\"amazing\", \"awesome\", \"yay\", \":)\", \":-)\", \":o)\", \":D\", \"=)\", \"=D\"]\n",
    "# sad = [\"no\", \"bad\", \"terrible\", \"$#@%\", \":(\", \":-(\", \":o(\", \"=(\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll stick to these simple indicators. Can you think of any problems that might arise if we used this model in the real world?\n",
    "\n",
    "*hint: things that are easy for humans to understand, but difficult for computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happy_tweets = []\n",
    "sad_tweets = []\n",
    "\n",
    "for tweet in random_tweets:            # for each tweet\n",
    "    for word in happy_indicators:         # we'll check if the tweet contains a word from our happy_indicators\n",
    "        if word in tweet:\n",
    "            happy_tweets.append(tweet)        # if it does, we'll add that tweet to our happy_tweets\n",
    "    for word in sad_indicators:        # repeat for sad\n",
    "        if word in tweet:\n",
    "            sad_tweets.append(tweet)\n",
    "\n",
    "print(\"Happy tweets: \" + str(len(happy_tweets)))\n",
    "\n",
    "print(\"Sad tweets: \" + str(len(sad_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
