{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocomplete  \n",
    "\n",
    "A model for predicting the next word in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to build an application that predicts the next word of a user's query - Naturally, this will involve dealing with probabilities. To build our predictor, we'll be implementing a Markov Model as our stochastic tool of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the details, let's take a look at the big picture. Our application will be taking in the user's input text, and will need some training set of data in order to sample probabilities of word sequences.  \n",
    "\n",
    "High-Level view of application:\n",
    "- Input: User's query, training set\n",
    "- Output: Next word  \n",
    "\n",
    "This leads us to make a function like *predict* below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(query, data):\n",
    "    # Return the most likely next word\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we'll be using the data to train the model that we're creating. And the model itself will be the thing that is used to determine the next word. Thus, we'll be implementing the more verbose version of *predict* below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(query, model):\n",
    "    # Return the most likely next word\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a version of a markov model that fits our problem statement. Each state in our markov chain will represent a unique word, and the probability that we go from state *A* to state *B* will represent the probability that the next word is *B* given the current word *A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Markov():\n",
    "    \n",
    "    def __init__(self, data=None):\n",
    "        self.word_history = {}\n",
    "        self.next_word_map = {}\n",
    "        self.data = data\n",
    "    \n",
    "    def train_word(self, word, next_word):\n",
    "        word = word.lower()\n",
    "        next_word = next_word.lower()\n",
    "        word_dict = self.word_history.setdefault(word, {next_word : 0})\n",
    "        word_dict.setdefault(next_word, 0)\n",
    "        word_dict[next_word] += 1\n",
    "        self.update()\n",
    "    \n",
    "    def train_set(self, words, domain=0):\n",
    "        # implement for data input in form of text file\n",
    "        # possible forms of words... .txt, string (sentence), etc\n",
    "        if domain == 0:\n",
    "            domain = len(words)-1\n",
    "        for i in range(domain):\n",
    "            self.train_word(words[i], words[i+1])\n",
    "    \n",
    "    def update(self):\n",
    "        for word in self.word_history:\n",
    "            most_probable = 0\n",
    "            best_next = \"...\"\n",
    "            for key, value in self.word_history[word].items():\n",
    "                if value > most_probable:\n",
    "                    best_next = key\n",
    "                    most_probable = value\n",
    "            self.next_word_map[word] = best_next\n",
    "    \n",
    "    def retrieve_next(self, word):\n",
    "        return self.next_word_map.setdefault(word, \"...\")\n",
    "\n",
    "    def probability(self, word, next_word):\n",
    "        word = word.lower()\n",
    "        next_word = next_word.lower()\n",
    "        total = sum([val for val in self.word_history[word].values()])\n",
    "        return 1.0*self.word_history[word][next_word]/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our Markov class to represent the Markov chain object that we need to create\n",
    "\n",
    "Class Methods  \n",
    "\n",
    "~~~~\n",
    "__init__\n",
    "    Initializes a new instance of a Markov chain\n",
    "\n",
    "train_word\n",
    "    trains the model using a pair of words\n",
    "\n",
    "train_set\n",
    "    trains the model using a list of words (multiple pairs of words)\n",
    "\n",
    "update\n",
    "    updates the stored probabilities after a new word is introduced\n",
    "\n",
    "retrieve_next\n",
    "    retrieves the next predicted word based on the previous word\n",
    "\n",
    "probability\n",
    "    returns the probability that next_word will follow word\n",
    "\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we incorporate this into our original function by updating *predict*. We simplify our task by focusing on the last word of the user's query.\n",
    "\n",
    "I.e., for a query \"Hi my name is\", we would look for the most likely word to follow \"is\".\n",
    "\n",
    "This simplification is made to accomodate for what our currently model is capable of, but we can see that a more powerful next-word-predictor would require a more sophisticated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(query, model):\n",
    "    words = query.split(\" \")\n",
    "    return model.retrieve_next(words[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've built our application, let's learn more about how it works by using it on some actual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model = Markov()\n",
    "test_model.train_word(\"hello\", \"everyone\")\n",
    "print(test_model.word_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_model.next_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model.train_word(\"hello\", \"world\")\n",
    "print(test_model.word_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model.train_word(\"hello\", \"world\")\n",
    "print(test_model.word_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_model.next_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model.train_word(\"goodbye\", \"everyone\")\n",
    "print(test_model.word_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_model.next_word_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained our model with a modest training set, but even this tiny sample size is enough for our model to start making some predictions.  \n",
    "\n",
    "After running the code above, we get the following counts:  \n",
    "\n",
    "hello -> world : 2  \n",
    "hello -> everyone : 1  \n",
    "goodbye -> everyone : 1  \n",
    "\n",
    "This is enough to tell us that if our last word was \"hello\", there is a 66% chance that the next word will be \"world\", and a 33% chance that the next word will be \"everyone\".  \n",
    "Similarly, a query of \"goodbye\" would result in \"everyone\" 100% of the time.  \n",
    "\n",
    "Since our application chooses the most likely next word, we can expect that given an input of \"hello\", it would ouput \"world\". Likewise, an input of \"goodbye\" would result in \"everyone\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Input: hello\")\n",
    "print(\"Output: \" + predict(\"hello\", test_model))\n",
    "print(\"Probability: \" + str(test_model.probability(\"hello\", \"world\")))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Input: goodbye\")\n",
    "print(\"Output: \" + predict(\"goodbye\", test_model))\n",
    "print(\"Probability: \" + str(test_model.probability(\"goodbye\", \"everyone\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tested the application with a smaller data set, let's use a more complete sequence of words to train our model. Looking for a corpus of text brings to mind NLTK, which we'll use to import the full text to the screenplay of Pirate of the Carribean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "pirates = webtext.words('pirates.txt') # creating a list where each element is a word (in order from the screenplay)\n",
    "\n",
    "new_model = Markov()\n",
    "new_model.train_set(pirates, 3000)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've taken the text of \"Pirates of the Caribbean\" and used the first 3000 words to train our model. Let's see what our model would predict the characters of this movie to say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input = Jack, Output = ???\n",
    "print(predict(\"jack\", new_model))\n",
    "print(\"Probability: \" + str(new_model.probability(\"jack\", predict(\"jack\", new_model))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what would be predicted if we were to continue predicting words to form a sentence... In other words, given the ability to generate the next word, we should be able to contruct a phrase or sentence as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"pirate\"\n",
    "print(\"Next word: \" + predict(query, new_model))\n",
    "print(\"Probability: \" + str(new_model.probability(query, predict(query, new_model))))\n",
    "\n",
    "query = query + \" \" + predict(query, new_model)\n",
    "print(\"Sentence: \" + query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Next word: \" + predict(query, new_model))\n",
    "\n",
    "query = query + \" \" + predict(query, new_model)\n",
    "print(\"Sentence: \" + query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Next word: \" + predict(query, new_model))\n",
    "\n",
    "query = query + \" \" + predict(query, new_model)\n",
    "print(\"Sentence: \" + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps: evaluate whether a sentence makes sense or not\n",
    "\n",
    "for each word in a sentence, use probability that the next word follows it\n",
    "\n",
    "given several sentence, which one makes the most sense? which one makes the least sense?\n",
    "\n",
    "further applications of NLTK?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
