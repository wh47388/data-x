{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Data-X Notebook: Word2vec\n",
    "#### Using NLP with Wrd2Vec in Python to do sentiment analysis on IMDB movie reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Following  steps: \n",
    "1. Reading of file labeledTrainData.tsv from data folder in a dataframe `train`.\n",
    "2. Clean the reviews in the input file.\n",
    "3. Use cleaned reviews to generate word vectors for this corpus.\n",
    "4. Train a classifier on Word vectors represntation of reviews for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data set\n",
    "\n",
    "The labeled training data set consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "* **testData** - The unlabeled test set. 25,000 rows containing an id, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\",delimiter=\"\\t\")\n",
    "# train.shape should be (25000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec3'></div>\n",
    "##  2. Cleaning the reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function called `review_cleaner` that reads in a review and:\n",
    "\n",
    "- Removes HTML tags (using beautifulsoup)\n",
    "- Removes non-letters (using regular expression)\n",
    "- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "- Removes all the English stopwords from the list of movie review words\n",
    "- Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def review_cleaner(review,lemmatize=True,stem=False):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "\n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    #1. Remove HTML tags\n",
    "    review = bs.BeautifulSoup(review).text\n",
    "\n",
    "    \n",
    "    #2. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #3. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #4.Set stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    clean_review=[]\n",
    "    for word in review:\n",
    "        if word not in eng_stopwords:\n",
    "            if lemmatize is True:\n",
    "                word=wnl.lemmatize(word)\n",
    "            elif stem is True:\n",
    "                if word == 'oed':\n",
    "                    continue\n",
    "                word=ps.stem(word)\n",
    "            clean_review.append(word)\n",
    "    return(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_reviews = len(train['review'])\n",
    "\n",
    "review_clean_original = []\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    review_clean_original.append(review_cleaner(train['review'][i]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_clean_original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "len(review_clean_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Words to Distributed representation i.e train Word2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "\n",
    "#ref: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sentences=review_clean_original\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # ignore all words with total frequency lower than this                       \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training word2vec model... \")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "           size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You can also use pretrained word2vec models that:\n",
    "#Download the Google pretrained model from,itâ€™s 1.5GB :\n",
    "#https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "#Once you donload save unzip the file and you should will get another zip file named\n",
    "#GoogleNews-vectors-negative300.bin. \n",
    "\n",
    "\n",
    "# Gmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Model Results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get vocabulary count of the model\n",
    "vocab_tmp = list(model.wv.vocab)\n",
    "print('Vocab length:',len(vocab_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Vocabulary words\n",
    "vocab_tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the Word embedding \n",
    "# model['stuff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get cosine similarity of words\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model.similarity('movie','film')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('actor','actress')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model.similarity('boring','dull')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['actor','male'], negative=['female'])\n",
    "# model.most_similar(positive=['king','woman'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"man woman ok kill\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"france man germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load the trained modelNumeric Representations of Words\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model with some semantic understanding of words, how should we use it? If you look beneath the hood, the Word2Vec model trained in earlier consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"wv.syn0\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get vocabulary count of the model\n",
    "vocab_tmp = list(model.wv.vocab)\n",
    "print('Vocab length:',len(vocab_tmp))\n",
    "\n",
    "# Get distributional representation of each word\n",
    "X = model[vocab_tmp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "# get two principle components of the feature space\n",
    "pca= decomposition.PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# set figure settings\n",
    "plt.figure(figsize=(10,10),dpi=100)\n",
    "\n",
    "# save pca values and vocab in dataframe df\n",
    "df = pd.concat([pd.DataFrame(pca),pd.Series(vocab_tmp)],axis=1)\n",
    "df.columns = ['x', 'y', 'word']\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Ist principal component\")\n",
    "plt.ylabel('2nd principal component')\n",
    "\n",
    "\n",
    "plt.scatter(x=pca[:, 0], y=pca[:, 1],s=3)\n",
    "for i, word in enumerate(df['word'][0:100]):\n",
    "    plt.annotate(word, (df['x'].iloc[i], df['y'].iloc[i]))\n",
    "plt.title(\"PCA Embedding\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A popular non-linear dimensionality reduction technique that preserves greatly thge local \n",
    "## and global structure of the data. Essentially tries to reconstruct the subspace in which the \n",
    "## data exists\n",
    "'''This will take time to run'''\n",
    "\n",
    "# from sklearn import manifold\n",
    "# tsne = manifold.TSNE(n_components=2)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# # set figure settings\n",
    "# plt.figure(figsize=(10,10),dpi=100)\n",
    "\n",
    "# # save pca values and vocab in dataframe df\n",
    "# df2 = pd.concat([pd.DataFrame(pca),pd.Series(vocab_tmp)],axis=1)\n",
    "# df2.columns = ['x', 'y', 'word']\n",
    "\n",
    "\n",
    "# plt.scatter(df2['x'][0:500], df2['y'][0:500],s=3)\n",
    "# for i, word in enumerate(df2['word'][0:500]):\n",
    "#     plt.annotate(word, (df2['x'].iloc[i], df2['y'].iloc[i]))\n",
    "# plt.title(\"Tsne Embedding\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Use Word Vectors to create a sentiment analysis model using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Averaging to get feature encoding of review:\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from earlier sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(review, model):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    featureVec =[]\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for n,word in enumerate(review):\n",
    "        if word in index2word_set: \n",
    "            featureVec.append(model[word])\n",
    "            \n",
    "    # Average the word vectors for a \n",
    "    featureVec = np.mean(featureVec,axis=0)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one \n",
    "    \n",
    "    reviewFeatureVecs = []\n",
    "    # Loop through the reviews\n",
    "    for counter,review in enumerate(reviews):\n",
    "        \n",
    "        # Print a status message every 5000th review\n",
    "        if counter%5000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        vector= makeFeatureVec(review, model)\n",
    "        reviewFeatureVecs.append(vector)\n",
    "            \n",
    "    return reviewFeatureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# # CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def train_sentiment(cleaned_reviews, y=train[\"sentiment\"],max_features=1000):\n",
    "    '''This function will:\n",
    "    1. Convert reviews into feature vectors using word2vec.\n",
    "    2. split data into train and test set.\n",
    "    3. train a random forest model using train n-gram counts and y (labels)\n",
    "    4. test the model on your test split\n",
    "    5. print accuracy of sentiment prediction on test and training data\n",
    "    6. print confusion matrix on test data results\n",
    "\n",
    "    To change n-gram type, set value of ngram argument\n",
    "    To change the number of features you want the countvectorizer to generate, set the value of max_features argument'''\n",
    "\n",
    "    print(\"1.Creating Feature vectors using word2vec...\\n\")\n",
    "\n",
    "    trainDataVecs = getAvgFeatureVecs( cleaned_reviews, model)\n",
    "    \n",
    "   \n",
    "    print(\"\\n2.Splitting dataset into train and test sets...\\n\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    trainDataVecs, y, random_state=0, test_size=.2)\n",
    "\n",
    "   \n",
    "    print(\"3. Training the random forest classifier...\\n\")\n",
    "    \n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "    \n",
    "    # Fit the forest to the training set, word2vecfeatures \n",
    "    # and the sentiment labels as the target variable\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(X_train)\n",
    "    test_predictions = forest.predict(X_test)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"=================Training Statistics======================\\n\")\n",
    "    print(\"The training accuracy is: \", train_acc)\n",
    "    print(\"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_sentiment(cleaned_reviews=review_clean_original, y=train[\"sentiment\"],max_features=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
